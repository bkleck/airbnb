{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from re import sub\n",
    "import multiprocessing\n",
    "from unidecode import unidecode\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7202016</td>\n",
       "      <td>38917982</td>\n",
       "      <td>2015-07-19</td>\n",
       "      <td>28943674</td>\n",
       "      <td>Bianca</td>\n",
       "      <td>Cute and cozy place. Perfect location to every...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39087409</td>\n",
       "      <td>2015-07-20</td>\n",
       "      <td>32440555</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Kelly has a great room in a very central locat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39820030</td>\n",
       "      <td>2015-07-26</td>\n",
       "      <td>37722850</td>\n",
       "      <td>Ian</td>\n",
       "      <td>Very spacious apartment, and in a great neighb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7202016</td>\n",
       "      <td>40813543</td>\n",
       "      <td>2015-08-02</td>\n",
       "      <td>33671805</td>\n",
       "      <td>George</td>\n",
       "      <td>Close to Seattle Center and all it has to offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7202016</td>\n",
       "      <td>41986501</td>\n",
       "      <td>2015-08-10</td>\n",
       "      <td>34959538</td>\n",
       "      <td>Ming</td>\n",
       "      <td>Kelly was a great host and very accommodating ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        id        date  reviewer_id reviewer_name  \\\n",
       "0     7202016  38917982  2015-07-19     28943674        Bianca   \n",
       "1     7202016  39087409  2015-07-20     32440555         Frank   \n",
       "2     7202016  39820030  2015-07-26     37722850           Ian   \n",
       "3     7202016  40813543  2015-08-02     33671805        George   \n",
       "4     7202016  41986501  2015-08-10     34959538          Ming   \n",
       "\n",
       "                                            comments  \n",
       "0  Cute and cozy place. Perfect location to every...  \n",
       "1  Kelly has a great room in a very central locat...  \n",
       "2  Very spacious apartment, and in a great neighb...  \n",
       "3  Close to Seattle Center and all it has to offe...  \n",
       "4  Kelly was a great host and very accommodating ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Boon Kong\\Desktop\\DSAI\\Project\\data\\reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Cleaning\n",
    "- Remove punctuations\n",
    "- Tokenize words (split review into list of words)\n",
    "- Remove stopwords (common words that do not add meaning to the context of the text)\n",
    "- Lemmatize words (reduce words to their root form to reduce vocabulary size and group words with same meaning together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for cleaning tasks\n",
    "\n",
    "def remove_punct(text):\n",
    "    no_punct = ''.join(char for char in text if char not in string.punctuation)\n",
    "    return no_punct\n",
    "\n",
    "# creates a list of words\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "# remove common words with no meaning e.g. connectors\n",
    "def remove_stopwords(token_list):\n",
    "    text = [word for word in token_list if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "# convert words into their root forms\n",
    "def lemmatize(text):\n",
    "    lemmatized_text = [wn.lemmatize(word) for word in text]\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7202016</td>\n",
       "      <td>38917982</td>\n",
       "      <td>2015-07-19</td>\n",
       "      <td>28943674</td>\n",
       "      <td>Bianca</td>\n",
       "      <td>Cute and cozy place. Perfect location to every...</td>\n",
       "      <td>Cute and cozy place Perfect location to everyt...</td>\n",
       "      <td>[cute, and, cozy, place, perfect, location, to...</td>\n",
       "      <td>[cute, cozy, place, perfect, location, everyth...</td>\n",
       "      <td>[cute, cozy, place, perfect, location, everyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39087409</td>\n",
       "      <td>2015-07-20</td>\n",
       "      <td>32440555</td>\n",
       "      <td>Frank</td>\n",
       "      <td>Kelly has a great room in a very central locat...</td>\n",
       "      <td>Kelly has a great room in a very central locat...</td>\n",
       "      <td>[kelly, has, a, great, room, in, a, very, cent...</td>\n",
       "      <td>[kelly, great, room, central, location, beauti...</td>\n",
       "      <td>[kelly, great, room, central, location, beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39820030</td>\n",
       "      <td>2015-07-26</td>\n",
       "      <td>37722850</td>\n",
       "      <td>Ian</td>\n",
       "      <td>Very spacious apartment, and in a great neighb...</td>\n",
       "      <td>Very spacious apartment and in a great neighbo...</td>\n",
       "      <td>[very, spacious, apartment, and, in, a, great,...</td>\n",
       "      <td>[spacious, apartment, great, neighborhood, kin...</td>\n",
       "      <td>[spacious, apartment, great, neighborhood, kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7202016</td>\n",
       "      <td>40813543</td>\n",
       "      <td>2015-08-02</td>\n",
       "      <td>33671805</td>\n",
       "      <td>George</td>\n",
       "      <td>Close to Seattle Center and all it has to offe...</td>\n",
       "      <td>Close to Seattle Center and all it has to offe...</td>\n",
       "      <td>[close, to, seattle, center, and, all, it, has...</td>\n",
       "      <td>[close, seattle, center, offer, ballet, theate...</td>\n",
       "      <td>[close, seattle, center, offer, ballet, theate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7202016</td>\n",
       "      <td>41986501</td>\n",
       "      <td>2015-08-10</td>\n",
       "      <td>34959538</td>\n",
       "      <td>Ming</td>\n",
       "      <td>Kelly was a great host and very accommodating ...</td>\n",
       "      <td>Kelly was a great host and very accommodating ...</td>\n",
       "      <td>[kelly, was, a, great, host, and, very, accomm...</td>\n",
       "      <td>[kelly, great, host, accommodating, great, nei...</td>\n",
       "      <td>[kelly, great, host, accommodating, great, nei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        id        date  reviewer_id reviewer_name  \\\n",
       "0     7202016  38917982  2015-07-19     28943674        Bianca   \n",
       "1     7202016  39087409  2015-07-20     32440555         Frank   \n",
       "2     7202016  39820030  2015-07-26     37722850           Ian   \n",
       "3     7202016  40813543  2015-08-02     33671805        George   \n",
       "4     7202016  41986501  2015-08-10     34959538          Ming   \n",
       "\n",
       "                                            comments  \\\n",
       "0  Cute and cozy place. Perfect location to every...   \n",
       "1  Kelly has a great room in a very central locat...   \n",
       "2  Very spacious apartment, and in a great neighb...   \n",
       "3  Close to Seattle Center and all it has to offe...   \n",
       "4  Kelly was a great host and very accommodating ...   \n",
       "\n",
       "                                       clean_comment  \\\n",
       "0  Cute and cozy place Perfect location to everyt...   \n",
       "1  Kelly has a great room in a very central locat...   \n",
       "2  Very spacious apartment and in a great neighbo...   \n",
       "3  Close to Seattle Center and all it has to offe...   \n",
       "4  Kelly was a great host and very accommodating ...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [cute, and, cozy, place, perfect, location, to...   \n",
       "1  [kelly, has, a, great, room, in, a, very, cent...   \n",
       "2  [very, spacious, apartment, and, in, a, great,...   \n",
       "3  [close, to, seattle, center, and, all, it, has...   \n",
       "4  [kelly, was, a, great, host, and, very, accomm...   \n",
       "\n",
       "                                        no_stopwords  \\\n",
       "0  [cute, cozy, place, perfect, location, everyth...   \n",
       "1  [kelly, great, room, central, location, beauti...   \n",
       "2  [spacious, apartment, great, neighborhood, kin...   \n",
       "3  [close, seattle, center, offer, ballet, theate...   \n",
       "4  [kelly, great, host, accommodating, great, nei...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [cute, cozy, place, perfect, location, everyth...  \n",
       "1  [kelly, great, room, central, location, beauti...  \n",
       "2  [spacious, apartment, great, neighborhood, kin...  \n",
       "3  [close, seattle, center, offer, ballet, theate...  \n",
       "4  [kelly, great, host, accommodating, great, nei...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments'] = df['comments'].astype(str)\n",
    "df['clean_comment'] = df['comments'].apply(lambda x: remove_punct(x))\n",
    "df['tokenized'] = df['clean_comment'].apply(lambda x: tokenize(x.lower()))\n",
    "df['no_stopwords'] = df['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "df['lemmatized'] = df['no_stopwords'].apply(lambda x: lemmatize(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmatized'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Gensim Word2Vec \n",
    "Gensim is an algorithm based on neural networks, using large amounts of unannotated plain text to learn relationship between words. This gives us a spatial dimension with words of the same meaning close to each other. For example, 'strong' and 'powerful' will be close to each other.\n",
    "\n",
    "\n",
    "## create word embeddings\n",
    "- 300 dimensional embeddings\n",
    "- lookup window = 4 (learn to predict word using 4 words from left and right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:31:13: collecting all words and their counts\n",
      "INFO - 21:31:13: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 21:31:16: PROGRESS: at sentence #50000, processed 1811173 words and 602816 word types\n",
      "INFO - 21:31:19: collected 885212 word types from a corpus of 3085563 words (unigram + bigrams) and 84849 sentences\n",
      "INFO - 21:31:19: using 885212 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 21:31:19: source_vocab length 885212\n",
      "INFO - 21:31:29: Phraser built with 32536 phrasegrams\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['kelly',\n",
       " 'great',\n",
       " 'room',\n",
       " 'central',\n",
       " 'location',\n",
       " 'beautiful',\n",
       " 'building',\n",
       " 'architecture',\n",
       " 'style',\n",
       " 'really',\n",
       " 'like',\n",
       " 'felt',\n",
       " 'guite',\n",
       " 'home',\n",
       " 'wish',\n",
       " 'spent',\n",
       " 'time',\n",
       " 'went',\n",
       " 'walk',\n",
       " 'found',\n",
       " 'seattle',\n",
       " 'center',\n",
       " 'major',\n",
       " 'food',\n",
       " 'festival',\n",
       " 'progress',\n",
       " 'treat',\n",
       " 'visited',\n",
       " 'space_needle',\n",
       " 'chihuly_glass',\n",
       " 'exhibit',\n",
       " 'pike_place',\n",
       " 'market',\n",
       " 'wow',\n",
       " 'thanks',\n",
       " 'great',\n",
       " 'stay']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put all words into a list\n",
    "sent = [row for row in df['lemmatized']]\n",
    "\n",
    "# create phrases from sentences\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:31:54: collecting all words and their counts\n",
      "INFO - 21:31:54: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 21:32:00: PROGRESS: at sentence #50000, processed 1542326 words, keeping 61718 word types\n",
      "INFO - 21:32:03: collected 79994 word types from a corpus of 2627642 raw words and 84849 sentences\n",
      "INFO - 21:32:03: Loading a fresh vocabulary\n",
      "INFO - 21:32:03: effective_min_count=3 retains 30260 unique words (37% of original 79994, drops 49734)\n",
      "INFO - 21:32:03: effective_min_count=3 leaves 2561531 word corpus (97% of original 2627642, drops 66111)\n",
      "INFO - 21:32:03: deleting the raw counts dictionary of 79994 items\n",
      "INFO - 21:32:03: sample=1e-05 downsamples 3054 most-common words\n",
      "INFO - 21:32:03: downsampling leaves estimated 677102 word corpus (26.4% of prior 2561531)\n",
      "INFO - 21:32:04: estimated required memory for 30260 words and 300 dimensions: 87754000 bytes\n",
      "INFO - 21:32:04: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.28 mins\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "start = time()\n",
    "\n",
    "model.build_vocab(sentences, progress_per=50000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:33:00: training model with 11 workers on 30260 vocabulary and 300 features, using sg=0 hs=0 sample=1e-05 negative=20 window=4\n",
      "INFO - 21:33:01: EPOCH 1 - PROGRESS: at 10.44% examples, 68541 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:33:02: EPOCH 1 - PROGRESS: at 20.48% examples, 68606 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:33:03: EPOCH 1 - PROGRESS: at 30.62% examples, 68200 words/s, in_qsize 0, out_qsize 3\n",
      "INFO - 21:33:04: EPOCH 1 - PROGRESS: at 34.89% examples, 58239 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 21:33:05: EPOCH 1 - PROGRESS: at 41.69% examples, 55308 words/s, in_qsize 20, out_qsize 0\n",
      "INFO - 21:33:06: EPOCH 1 - PROGRESS: at 51.77% examples, 56798 words/s, in_qsize 17, out_qsize 2\n",
      "INFO - 21:33:07: EPOCH 1 - PROGRESS: at 62.13% examples, 58030 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:33:08: EPOCH 1 - PROGRESS: at 73.55% examples, 60094 words/s, in_qsize 18, out_qsize 0\n",
      "INFO - 21:33:10: EPOCH 1 - PROGRESS: at 84.01% examples, 60062 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:33:10: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:33:10: EPOCH - 1 : training on 2627642 raw words (676180 effective words) took 10.0s, 67930 effective words/s\n",
      "INFO - 21:33:11: EPOCH 2 - PROGRESS: at 10.43% examples, 65890 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:33:12: EPOCH 2 - PROGRESS: at 20.48% examples, 67342 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:33:13: EPOCH 2 - PROGRESS: at 24.64% examples, 53515 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 21:33:14: EPOCH 2 - PROGRESS: at 32.16% examples, 52112 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:33:16: EPOCH 2 - PROGRESS: at 45.05% examples, 58003 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 21:33:17: EPOCH 2 - PROGRESS: at 56.83% examples, 59970 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 21:33:18: EPOCH 2 - PROGRESS: at 63.95% examples, 58357 words/s, in_qsize 15, out_qsize 6\n",
      "INFO - 21:33:19: EPOCH 2 - PROGRESS: at 73.48% examples, 58799 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:33:20: EPOCH 2 - PROGRESS: at 83.13% examples, 59616 words/s, in_qsize 20, out_qsize 2\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:33:20: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:33:20: EPOCH - 2 : training on 2627642 raw words (677005 effective words) took 10.0s, 67372 effective words/s\n",
      "INFO - 21:33:21: EPOCH 3 - PROGRESS: at 8.07% examples, 48625 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:33:22: EPOCH 3 - PROGRESS: at 10.83% examples, 33763 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:33:23: EPOCH 3 - PROGRESS: at 20.13% examples, 43337 words/s, in_qsize 16, out_qsize 7\n",
      "INFO - 21:33:25: EPOCH 3 - PROGRESS: at 39.81% examples, 60526 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 21:33:26: EPOCH 3 - PROGRESS: at 43.91% examples, 54306 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:33:27: EPOCH 3 - PROGRESS: at 54.63% examples, 56226 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:33:28: EPOCH 3 - PROGRESS: at 64.39% examples, 56708 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:33:29: EPOCH 3 - PROGRESS: at 74.25% examples, 57797 words/s, in_qsize 16, out_qsize 6\n",
      "INFO - 21:33:30: EPOCH 3 - PROGRESS: at 85.46% examples, 59360 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:33:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:33:30: EPOCH - 3 : training on 2627642 raw words (676820 effective words) took 10.1s, 66963 effective words/s\n",
      "INFO - 21:33:31: EPOCH 4 - PROGRESS: at 10.43% examples, 67270 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:33:32: EPOCH 4 - PROGRESS: at 20.48% examples, 68031 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:33:34: EPOCH 4 - PROGRESS: at 30.98% examples, 68458 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:33:35: EPOCH 4 - PROGRESS: at 41.32% examples, 68484 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:33:36: EPOCH 4 - PROGRESS: at 51.67% examples, 67941 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 21:33:37: EPOCH 4 - PROGRESS: at 62.08% examples, 67793 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 21:33:38: EPOCH 4 - PROGRESS: at 65.89% examples, 61132 words/s, in_qsize 12, out_qsize 1\n",
      "INFO - 21:33:39: EPOCH 4 - PROGRESS: at 72.77% examples, 59162 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 21:33:40: EPOCH 4 - PROGRESS: at 85.09% examples, 60963 words/s, in_qsize 20, out_qsize 0\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:33:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:33:40: EPOCH - 4 : training on 2627642 raw words (676671 effective words) took 9.9s, 68223 effective words/s\n",
      "INFO - 21:33:41: EPOCH 5 - PROGRESS: at 8.83% examples, 58968 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 21:33:42: EPOCH 5 - PROGRESS: at 19.43% examples, 64512 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:33:43: EPOCH 5 - PROGRESS: at 21.98% examples, 48986 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 21:33:44: EPOCH 5 - PROGRESS: at 31.09% examples, 52085 words/s, in_qsize 16, out_qsize 2\n",
      "INFO - 21:33:45: EPOCH 5 - PROGRESS: at 36.44% examples, 48328 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:33:47: EPOCH 5 - PROGRESS: at 45.44% examples, 50114 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 21:33:48: EPOCH 5 - PROGRESS: at 56.44% examples, 52151 words/s, in_qsize 21, out_qsize 1\n",
      "INFO - 21:33:49: EPOCH 5 - PROGRESS: at 68.23% examples, 55408 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 21:33:50: EPOCH 5 - PROGRESS: at 80.18% examples, 57885 words/s, in_qsize 12, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:33:51: EPOCH 5 - PROGRESS: at 88.76% examples, 57355 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:33:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:33:51: EPOCH - 5 : training on 2627642 raw words (677390 effective words) took 10.7s, 63537 effective words/s\n",
      "INFO - 21:33:52: EPOCH 6 - PROGRESS: at 10.43% examples, 69292 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:33:53: EPOCH 6 - PROGRESS: at 20.48% examples, 69136 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:33:54: EPOCH 6 - PROGRESS: at 30.27% examples, 67886 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:33:55: EPOCH 6 - PROGRESS: at 40.93% examples, 67882 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:33:56: EPOCH 6 - PROGRESS: at 51.27% examples, 67988 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:33:57: EPOCH 6 - PROGRESS: at 60.60% examples, 66618 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:33:58: EPOCH 6 - PROGRESS: at 70.50% examples, 66163 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:34:00: EPOCH 6 - PROGRESS: at 75.01% examples, 59508 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 21:34:01: EPOCH 6 - PROGRESS: at 84.34% examples, 59910 words/s, in_qsize 19, out_qsize 0\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:34:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:34:01: EPOCH - 6 : training on 2627642 raw words (677094 effective words) took 10.1s, 67151 effective words/s\n",
      "INFO - 21:34:02: EPOCH 7 - PROGRESS: at 9.20% examples, 59994 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:34:03: EPOCH 7 - PROGRESS: at 12.48% examples, 40351 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 21:34:04: EPOCH 7 - PROGRESS: at 20.55% examples, 45542 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:34:05: EPOCH 7 - PROGRESS: at 32.15% examples, 52263 words/s, in_qsize 20, out_qsize 0\n",
      "INFO - 21:34:07: EPOCH 7 - PROGRESS: at 43.86% examples, 54198 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:34:08: EPOCH 7 - PROGRESS: at 55.31% examples, 56889 words/s, in_qsize 19, out_qsize 1\n",
      "INFO - 21:34:09: EPOCH 7 - PROGRESS: at 64.35% examples, 56749 words/s, in_qsize 17, out_qsize 6\n",
      "INFO - 21:34:10: EPOCH 7 - PROGRESS: at 76.46% examples, 58761 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:34:11: EPOCH 7 - PROGRESS: at 87.42% examples, 59595 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:34:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:34:11: EPOCH - 7 : training on 2627642 raw words (676655 effective words) took 10.1s, 67150 effective words/s\n",
      "INFO - 21:34:12: EPOCH 8 - PROGRESS: at 10.03% examples, 65529 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:34:13: EPOCH 8 - PROGRESS: at 14.22% examples, 44053 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 21:34:14: EPOCH 8 - PROGRESS: at 26.35% examples, 56153 words/s, in_qsize 6, out_qsize 3\n",
      "INFO - 21:34:16: EPOCH 8 - PROGRESS: at 36.49% examples, 57330 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 21:34:17: EPOCH 8 - PROGRESS: at 43.59% examples, 54904 words/s, in_qsize 15, out_qsize 8\n",
      "INFO - 21:34:18: EPOCH 8 - PROGRESS: at 60.18% examples, 63328 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 21:34:19: EPOCH 8 - PROGRESS: at 67.37% examples, 59390 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:34:20: EPOCH 8 - PROGRESS: at 77.29% examples, 60095 words/s, in_qsize 13, out_qsize 9\n",
      "INFO - 21:34:21: EPOCH 8 - PROGRESS: at 89.55% examples, 62449 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:34:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:34:21: EPOCH - 8 : training on 2627642 raw words (677040 effective words) took 9.8s, 68810 effective words/s\n",
      "INFO - 21:34:22: EPOCH 9 - PROGRESS: at 5.01% examples, 31029 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:34:23: EPOCH 9 - PROGRESS: at 11.22% examples, 35710 words/s, in_qsize 20, out_qsize 0\n",
      "INFO - 21:34:24: EPOCH 9 - PROGRESS: at 20.49% examples, 44416 words/s, in_qsize 21, out_qsize 2\n",
      "INFO - 21:34:25: EPOCH 9 - PROGRESS: at 32.96% examples, 54070 words/s, in_qsize 12, out_qsize 4\n",
      "INFO - 21:34:26: EPOCH 9 - PROGRESS: at 45.48% examples, 59852 words/s, in_qsize 8, out_qsize 1\n",
      "INFO - 21:34:27: EPOCH 9 - PROGRESS: at 53.33% examples, 58214 words/s, in_qsize 17, out_qsize 0\n",
      "INFO - 21:34:28: EPOCH 9 - PROGRESS: at 63.20% examples, 59210 words/s, in_qsize 16, out_qsize 2\n",
      "INFO - 21:34:29: EPOCH 9 - PROGRESS: at 72.77% examples, 59429 words/s, in_qsize 20, out_qsize 0\n",
      "INFO - 21:34:30: EPOCH 9 - PROGRESS: at 82.84% examples, 59867 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:34:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:34:31: EPOCH - 9 : training on 2627642 raw words (677398 effective words) took 10.0s, 67840 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:34:32: EPOCH 10 - PROGRESS: at 10.44% examples, 64940 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:34:33: EPOCH 10 - PROGRESS: at 20.89% examples, 67647 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:34:34: EPOCH 10 - PROGRESS: at 30.29% examples, 66198 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 21:34:35: EPOCH 10 - PROGRESS: at 35.30% examples, 54825 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:34:37: EPOCH 10 - PROGRESS: at 42.44% examples, 52077 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:34:38: EPOCH 10 - PROGRESS: at 52.55% examples, 53869 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:34:39: EPOCH 10 - PROGRESS: at 62.77% examples, 55502 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 21:34:40: EPOCH 10 - PROGRESS: at 73.14% examples, 56932 words/s, in_qsize 21, out_qsize 1\n",
      "INFO - 21:34:41: EPOCH 10 - PROGRESS: at 84.00% examples, 58655 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:34:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:34:41: EPOCH - 10 : training on 2627642 raw words (676677 effective words) took 10.2s, 66164 effective words/s\n",
      "INFO - 21:34:42: EPOCH 11 - PROGRESS: at 10.05% examples, 66245 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:34:43: EPOCH 11 - PROGRESS: at 20.87% examples, 70028 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:34:45: EPOCH 11 - PROGRESS: at 27.70% examples, 59894 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 21:34:46: EPOCH 11 - PROGRESS: at 32.24% examples, 52654 words/s, in_qsize 17, out_qsize 3\n",
      "INFO - 21:34:47: EPOCH 11 - PROGRESS: at 42.38% examples, 55726 words/s, in_qsize 20, out_qsize 0\n",
      "INFO - 21:34:48: EPOCH 11 - PROGRESS: at 55.39% examples, 57473 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:34:49: EPOCH 11 - PROGRESS: at 70.48% examples, 61674 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 21:34:50: EPOCH 11 - PROGRESS: at 78.72% examples, 60128 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:34:51: EPOCH 11 - PROGRESS: at 92.27% examples, 63423 words/s, in_qsize 13, out_qsize 8\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:34:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:34:51: EPOCH - 11 : training on 2627642 raw words (676998 effective words) took 9.9s, 68357 effective words/s\n",
      "INFO - 21:34:52: EPOCH 12 - PROGRESS: at 4.66% examples, 30490 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 21:34:54: EPOCH 12 - PROGRESS: at 12.87% examples, 36725 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:34:55: EPOCH 12 - PROGRESS: at 22.76% examples, 46566 words/s, in_qsize 17, out_qsize 4\n",
      "INFO - 21:34:56: EPOCH 12 - PROGRESS: at 32.55% examples, 51234 words/s, in_qsize 21, out_qsize 1\n",
      "INFO - 21:34:57: EPOCH 12 - PROGRESS: at 44.25% examples, 55202 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:34:58: EPOCH 12 - PROGRESS: at 54.57% examples, 56613 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:34:59: EPOCH 12 - PROGRESS: at 64.64% examples, 58112 words/s, in_qsize 20, out_qsize 2\n",
      "INFO - 21:35:00: EPOCH 12 - PROGRESS: at 76.10% examples, 59295 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:01: EPOCH 12 - PROGRESS: at 88.58% examples, 61911 words/s, in_qsize 9, out_qsize 5\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:35:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:35:01: EPOCH - 12 : training on 2627642 raw words (676895 effective words) took 9.9s, 68117 effective words/s\n",
      "INFO - 21:35:02: EPOCH 13 - PROGRESS: at 10.02% examples, 64943 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:35:03: EPOCH 13 - PROGRESS: at 20.87% examples, 68246 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:35:04: EPOCH 13 - PROGRESS: at 31.35% examples, 69495 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:35:05: EPOCH 13 - PROGRESS: at 41.32% examples, 68684 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:35:07: EPOCH 13 - PROGRESS: at 50.09% examples, 63750 words/s, in_qsize 3, out_qsize 0\n",
      "INFO - 21:35:08: EPOCH 13 - PROGRESS: at 54.60% examples, 57937 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 21:35:09: EPOCH 13 - PROGRESS: at 66.99% examples, 60442 words/s, in_qsize 19, out_qsize 0\n",
      "INFO - 21:35:10: EPOCH 13 - PROGRESS: at 80.55% examples, 62388 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 21:35:11: EPOCH 13 - PROGRESS: at 90.72% examples, 62877 words/s, in_qsize 19, out_qsize 6\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:35:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:35:11: EPOCH - 13 : training on 2627642 raw words (677652 effective words) took 9.9s, 68742 effective words/s\n",
      "INFO - 21:35:12: EPOCH 14 - PROGRESS: at 10.43% examples, 67925 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:35:13: EPOCH 14 - PROGRESS: at 20.48% examples, 67588 words/s, in_qsize 1, out_qsize 2\n",
      "INFO - 21:35:14: EPOCH 14 - PROGRESS: at 30.62% examples, 67440 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 21:35:15: EPOCH 14 - PROGRESS: at 42.06% examples, 68783 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:35:16: EPOCH 14 - PROGRESS: at 52.49% examples, 68682 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:35:17: EPOCH 14 - PROGRESS: at 62.80% examples, 68512 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 21:35:18: EPOCH 14 - PROGRESS: at 73.89% examples, 69049 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:35:19: EPOCH 14 - PROGRESS: at 81.70% examples, 66318 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:35:20: EPOCH 14 - PROGRESS: at 85.37% examples, 61790 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:21: worker thread finished; awaiting finish of 10 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:35:21: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:35:21: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:35:21: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:35:21: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:35:21: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:35:21: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:35:21: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:35:21: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:35:21: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:35:21: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:35:21: EPOCH - 14 : training on 2627642 raw words (677495 effective words) took 9.7s, 69534 effective words/s\n",
      "INFO - 21:35:22: EPOCH 15 - PROGRESS: at 9.60% examples, 61058 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:35:23: EPOCH 15 - PROGRESS: at 13.26% examples, 40609 words/s, in_qsize 16, out_qsize 0\n",
      "INFO - 21:35:24: EPOCH 15 - PROGRESS: at 21.25% examples, 45806 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:25: EPOCH 15 - PROGRESS: at 34.48% examples, 54141 words/s, in_qsize 17, out_qsize 0\n",
      "INFO - 21:35:26: EPOCH 15 - PROGRESS: at 43.86% examples, 54837 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:27: EPOCH 15 - PROGRESS: at 53.71% examples, 55883 words/s, in_qsize 17, out_qsize 5\n",
      "INFO - 21:35:28: EPOCH 15 - PROGRESS: at 65.81% examples, 59304 words/s, in_qsize 17, out_qsize 1\n",
      "INFO - 21:35:29: EPOCH 15 - PROGRESS: at 76.21% examples, 60385 words/s, in_qsize 13, out_qsize 3\n",
      "INFO - 21:35:30: EPOCH 15 - PROGRESS: at 83.60% examples, 59364 words/s, in_qsize 15, out_qsize 9\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:35:31: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:35:31: EPOCH - 15 : training on 2627642 raw words (678289 effective words) took 10.0s, 67641 effective words/s\n",
      "INFO - 21:35:32: EPOCH 16 - PROGRESS: at 6.22% examples, 40460 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 21:35:33: EPOCH 16 - PROGRESS: at 17.24% examples, 57191 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:35:34: EPOCH 16 - PROGRESS: at 26.35% examples, 58646 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:35:35: EPOCH 16 - PROGRESS: at 29.64% examples, 49031 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 21:35:36: EPOCH 16 - PROGRESS: at 37.20% examples, 49242 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:37: EPOCH 16 - PROGRESS: at 48.13% examples, 52326 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:35:38: EPOCH 16 - PROGRESS: at 58.71% examples, 54334 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:39: EPOCH 16 - PROGRESS: at 68.03% examples, 55503 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:35:40: EPOCH 16 - PROGRESS: at 81.67% examples, 59048 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:35:41: EPOCH 16 - PROGRESS: at 96.84% examples, 63215 words/s, in_qsize 7, out_qsize 5\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:35:41: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:35:41: EPOCH - 16 : training on 2627642 raw words (676654 effective words) took 10.4s, 65160 effective words/s\n",
      "INFO - 21:35:42: EPOCH 17 - PROGRESS: at 9.60% examples, 63950 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:35:43: EPOCH 17 - PROGRESS: at 13.16% examples, 43570 words/s, in_qsize 10, out_qsize 0\n",
      "INFO - 21:35:44: EPOCH 17 - PROGRESS: at 19.79% examples, 43494 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:35:45: EPOCH 17 - PROGRESS: at 31.40% examples, 52252 words/s, in_qsize 16, out_qsize 1\n",
      "INFO - 21:35:46: EPOCH 17 - PROGRESS: at 40.59% examples, 53523 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:48: EPOCH 17 - PROGRESS: at 50.51% examples, 55343 words/s, in_qsize 19, out_qsize 3\n",
      "INFO - 21:35:49: EPOCH 17 - PROGRESS: at 60.97% examples, 57120 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:35:50: EPOCH 17 - PROGRESS: at 72.03% examples, 58711 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:51: EPOCH 17 - PROGRESS: at 83.23% examples, 60429 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:35:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:35:51: EPOCH - 17 : training on 2627642 raw words (677884 effective words) took 10.0s, 67982 effective words/s\n",
      "INFO - 21:35:52: EPOCH 18 - PROGRESS: at 7.35% examples, 43644 words/s, in_qsize 2, out_qsize 0\n",
      "INFO - 21:35:53: EPOCH 18 - PROGRESS: at 11.22% examples, 34870 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:35:55: EPOCH 18 - PROGRESS: at 24.30% examples, 47223 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:56: EPOCH 18 - PROGRESS: at 34.48% examples, 52101 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:57: EPOCH 18 - PROGRESS: at 46.26% examples, 56888 words/s, in_qsize 18, out_qsize 0\n",
      "INFO - 21:35:58: EPOCH 18 - PROGRESS: at 55.74% examples, 56815 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:35:59: EPOCH 18 - PROGRESS: at 67.37% examples, 58190 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:36:00: EPOCH 18 - PROGRESS: at 77.31% examples, 59021 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 21:36:01: EPOCH 18 - PROGRESS: at 86.88% examples, 59775 words/s, in_qsize 18, out_qsize 4\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:36:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:36:01: EPOCH - 18 : training on 2627642 raw words (676172 effective words) took 10.0s, 67350 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:36:02: EPOCH 19 - PROGRESS: at 10.02% examples, 66091 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:36:04: EPOCH 19 - PROGRESS: at 14.29% examples, 39972 words/s, in_qsize 18, out_qsize 2\n",
      "INFO - 21:36:05: EPOCH 19 - PROGRESS: at 23.87% examples, 47900 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:36:06: EPOCH 19 - PROGRESS: at 33.30% examples, 51557 words/s, in_qsize 19, out_qsize 3\n",
      "INFO - 21:36:07: EPOCH 19 - PROGRESS: at 43.53% examples, 54813 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 21:36:08: EPOCH 19 - PROGRESS: at 54.88% examples, 57261 words/s, in_qsize 14, out_qsize 5\n",
      "INFO - 21:36:09: EPOCH 19 - PROGRESS: at 70.39% examples, 61495 words/s, in_qsize 10, out_qsize 1\n",
      "INFO - 21:36:10: EPOCH 19 - PROGRESS: at 78.33% examples, 59723 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:36:11: EPOCH 19 - PROGRESS: at 89.54% examples, 61452 words/s, in_qsize 17, out_qsize 4\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:36:11: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:36:11: EPOCH - 19 : training on 2627642 raw words (677055 effective words) took 10.0s, 67678 effective words/s\n",
      "INFO - 21:36:12: EPOCH 20 - PROGRESS: at 10.43% examples, 67877 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:36:13: EPOCH 20 - PROGRESS: at 19.79% examples, 64875 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:36:14: EPOCH 20 - PROGRESS: at 29.16% examples, 64742 words/s, in_qsize 1, out_qsize 2\n",
      "INFO - 21:36:16: EPOCH 20 - PROGRESS: at 39.84% examples, 65763 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:36:17: EPOCH 20 - PROGRESS: at 51.27% examples, 67029 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:36:18: EPOCH 20 - PROGRESS: at 56.48% examples, 61167 words/s, in_qsize 6, out_qsize 0\n",
      "INFO - 21:36:19: EPOCH 20 - PROGRESS: at 62.09% examples, 57319 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 21:36:20: EPOCH 20 - PROGRESS: at 72.03% examples, 56936 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:36:21: EPOCH 20 - PROGRESS: at 88.76% examples, 60120 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:36:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:36:22: EPOCH - 20 : training on 2627642 raw words (677664 effective words) took 10.2s, 66115 effective words/s\n",
      "INFO - 21:36:23: EPOCH 21 - PROGRESS: at 10.02% examples, 59278 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:36:24: EPOCH 21 - PROGRESS: at 12.76% examples, 38893 words/s, in_qsize 20, out_qsize 0\n",
      "INFO - 21:36:25: EPOCH 21 - PROGRESS: at 22.44% examples, 47541 words/s, in_qsize 19, out_qsize 2\n",
      "INFO - 21:36:26: EPOCH 21 - PROGRESS: at 33.31% examples, 53519 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:36:27: EPOCH 21 - PROGRESS: at 50.09% examples, 60908 words/s, in_qsize 7, out_qsize 0\n",
      "INFO - 21:36:28: EPOCH 21 - PROGRESS: at 57.21% examples, 57639 words/s, in_qsize 20, out_qsize 0\n",
      "INFO - 21:36:29: EPOCH 21 - PROGRESS: at 66.26% examples, 57727 words/s, in_qsize 21, out_qsize 2\n",
      "INFO - 21:36:31: EPOCH 21 - PROGRESS: at 78.00% examples, 59287 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:36:32: EPOCH 21 - PROGRESS: at 87.77% examples, 59591 words/s, in_qsize 19, out_qsize 10\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:36:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:36:32: EPOCH - 21 : training on 2627642 raw words (676520 effective words) took 10.1s, 67180 effective words/s\n",
      "INFO - 21:36:33: EPOCH 22 - PROGRESS: at 10.43% examples, 66814 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:36:34: EPOCH 22 - PROGRESS: at 19.43% examples, 62950 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:36:35: EPOCH 22 - PROGRESS: at 22.39% examples, 48290 words/s, in_qsize 20, out_qsize 0\n",
      "INFO - 21:36:36: EPOCH 22 - PROGRESS: at 32.15% examples, 51637 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:36:37: EPOCH 22 - PROGRESS: at 44.30% examples, 56779 words/s, in_qsize 18, out_qsize 0\n",
      "INFO - 21:36:38: EPOCH 22 - PROGRESS: at 53.35% examples, 56739 words/s, in_qsize 16, out_qsize 6\n",
      "INFO - 21:36:39: EPOCH 22 - PROGRESS: at 65.11% examples, 59588 words/s, in_qsize 18, out_qsize 0\n",
      "INFO - 21:36:40: EPOCH 22 - PROGRESS: at 83.23% examples, 66951 words/s, in_qsize 1, out_qsize 2\n",
      "INFO - 21:36:41: EPOCH 22 - PROGRESS: at 92.63% examples, 66528 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:36:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:36:42: EPOCH - 22 : training on 2627642 raw words (676880 effective words) took 9.9s, 68258 effective words/s\n",
      "INFO - 21:36:43: EPOCH 23 - PROGRESS: at 10.44% examples, 64270 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:36:44: EPOCH 23 - PROGRESS: at 21.25% examples, 67284 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:36:45: EPOCH 23 - PROGRESS: at 32.15% examples, 69127 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:36:46: EPOCH 23 - PROGRESS: at 41.69% examples, 67676 words/s, in_qsize 0, out_qsize 3\n",
      "INFO - 21:36:47: EPOCH 23 - PROGRESS: at 46.21% examples, 60437 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 21:36:48: EPOCH 23 - PROGRESS: at 53.25% examples, 57034 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:36:49: EPOCH 23 - PROGRESS: at 63.99% examples, 58899 words/s, in_qsize 11, out_qsize 8\n",
      "INFO - 21:36:50: EPOCH 23 - PROGRESS: at 73.91% examples, 59428 words/s, in_qsize 18, out_qsize 4\n",
      "INFO - 21:36:51: EPOCH 23 - PROGRESS: at 82.74% examples, 59755 words/s, in_qsize 13, out_qsize 8\n",
      "INFO - 21:36:52: worker thread finished; awaiting finish of 10 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:36:52: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:36:52: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:36:52: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:36:52: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:36:52: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:36:52: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:36:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:36:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:36:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:36:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:36:52: EPOCH - 23 : training on 2627642 raw words (678308 effective words) took 10.1s, 67463 effective words/s\n",
      "INFO - 21:36:53: EPOCH 24 - PROGRESS: at 9.60% examples, 62174 words/s, in_qsize 0, out_qsize 3\n",
      "INFO - 21:36:54: EPOCH 24 - PROGRESS: at 19.07% examples, 57748 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:36:55: EPOCH 24 - PROGRESS: at 22.42% examples, 47102 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 21:36:56: EPOCH 24 - PROGRESS: at 41.32% examples, 64989 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:36:57: EPOCH 24 - PROGRESS: at 44.62% examples, 56598 words/s, in_qsize 19, out_qsize 0\n",
      "INFO - 21:36:58: EPOCH 24 - PROGRESS: at 56.12% examples, 56838 words/s, in_qsize 21, out_qsize 1\n",
      "INFO - 21:36:59: EPOCH 24 - PROGRESS: at 66.17% examples, 58479 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:37:00: EPOCH 24 - PROGRESS: at 79.10% examples, 61731 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 21:37:01: EPOCH 24 - PROGRESS: at 86.27% examples, 60346 words/s, in_qsize 19, out_qsize 0\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:37:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:37:02: EPOCH - 24 : training on 2627642 raw words (677098 effective words) took 10.1s, 67372 effective words/s\n",
      "INFO - 21:37:03: EPOCH 25 - PROGRESS: at 10.43% examples, 66997 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:37:04: EPOCH 25 - PROGRESS: at 21.25% examples, 69201 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:37:05: EPOCH 25 - PROGRESS: at 31.76% examples, 70016 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:37:06: EPOCH 25 - PROGRESS: at 42.06% examples, 69597 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:37:07: EPOCH 25 - PROGRESS: at 52.13% examples, 68828 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:37:08: EPOCH 25 - PROGRESS: at 62.45% examples, 67337 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 21:37:09: EPOCH 25 - PROGRESS: at 64.73% examples, 59818 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:37:10: EPOCH 25 - PROGRESS: at 77.67% examples, 62688 words/s, in_qsize 14, out_qsize 0\n",
      "INFO - 21:37:11: EPOCH 25 - PROGRESS: at 85.50% examples, 61494 words/s, in_qsize 20, out_qsize 0\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:37:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:37:12: EPOCH - 25 : training on 2627642 raw words (677852 effective words) took 9.8s, 68967 effective words/s\n",
      "INFO - 21:37:13: EPOCH 26 - PROGRESS: at 7.33% examples, 40165 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 21:37:14: EPOCH 26 - PROGRESS: at 11.56% examples, 34743 words/s, in_qsize 17, out_qsize 6\n",
      "INFO - 21:37:15: EPOCH 26 - PROGRESS: at 22.39% examples, 47028 words/s, in_qsize 21, out_qsize 1\n",
      "INFO - 21:37:16: EPOCH 26 - PROGRESS: at 31.35% examples, 48561 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:37:17: EPOCH 26 - PROGRESS: at 42.40% examples, 53232 words/s, in_qsize 17, out_qsize 1\n",
      "INFO - 21:37:18: EPOCH 26 - PROGRESS: at 53.32% examples, 54270 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:37:19: EPOCH 26 - PROGRESS: at 65.80% examples, 57905 words/s, in_qsize 15, out_qsize 0\n",
      "INFO - 21:37:20: EPOCH 26 - PROGRESS: at 72.80% examples, 56316 words/s, in_qsize 15, out_qsize 8\n",
      "INFO - 21:37:22: EPOCH 26 - PROGRESS: at 84.74% examples, 57989 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:37:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:37:22: EPOCH - 26 : training on 2627642 raw words (676567 effective words) took 10.3s, 65591 effective words/s\n",
      "INFO - 21:37:23: EPOCH 27 - PROGRESS: at 10.01% examples, 64934 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 21:37:24: EPOCH 27 - PROGRESS: at 20.48% examples, 67785 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:37:25: EPOCH 27 - PROGRESS: at 29.88% examples, 65800 words/s, in_qsize 0, out_qsize 2\n",
      "INFO - 21:37:26: EPOCH 27 - PROGRESS: at 41.32% examples, 68485 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:37:27: EPOCH 27 - PROGRESS: at 52.12% examples, 68886 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:37:28: EPOCH 27 - PROGRESS: at 56.83% examples, 61139 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 21:37:29: EPOCH 27 - PROGRESS: at 63.58% examples, 58823 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:37:30: EPOCH 27 - PROGRESS: at 75.36% examples, 61190 words/s, in_qsize 16, out_qsize 2\n",
      "INFO - 21:37:32: EPOCH 27 - PROGRESS: at 88.10% examples, 61646 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:37:32: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:37:32: EPOCH - 27 : training on 2627642 raw words (677274 effective words) took 9.8s, 68799 effective words/s\n",
      "INFO - 21:37:33: EPOCH 28 - PROGRESS: at 10.01% examples, 63901 words/s, in_qsize 0, out_qsize 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:37:34: EPOCH 28 - PROGRESS: at 20.86% examples, 68413 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:37:35: EPOCH 28 - PROGRESS: at 28.03% examples, 58406 words/s, in_qsize 5, out_qsize 0\n",
      "INFO - 21:37:36: EPOCH 28 - PROGRESS: at 32.96% examples, 52469 words/s, in_qsize 19, out_qsize 1\n",
      "INFO - 21:37:37: EPOCH 28 - PROGRESS: at 44.65% examples, 55285 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:37:38: EPOCH 28 - PROGRESS: at 56.41% examples, 58097 words/s, in_qsize 18, out_qsize 1\n",
      "INFO - 21:37:40: EPOCH 28 - PROGRESS: at 66.55% examples, 58607 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:37:41: EPOCH 28 - PROGRESS: at 76.84% examples, 59324 words/s, in_qsize 18, out_qsize 5\n",
      "INFO - 21:37:42: EPOCH 28 - PROGRESS: at 89.55% examples, 62178 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:37:42: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:37:42: EPOCH - 28 : training on 2627642 raw words (677757 effective words) took 9.9s, 68501 effective words/s\n",
      "INFO - 21:37:43: EPOCH 29 - PROGRESS: at 9.60% examples, 63478 words/s, in_qsize 0, out_qsize 3\n",
      "INFO - 21:37:44: EPOCH 29 - PROGRESS: at 14.27% examples, 46884 words/s, in_qsize 11, out_qsize 0\n",
      "INFO - 21:37:45: EPOCH 29 - PROGRESS: at 19.81% examples, 44157 words/s, in_qsize 16, out_qsize 6\n",
      "INFO - 21:37:46: EPOCH 29 - PROGRESS: at 30.95% examples, 52021 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:37:47: EPOCH 29 - PROGRESS: at 42.45% examples, 55786 words/s, in_qsize 20, out_qsize 1\n",
      "INFO - 21:37:48: EPOCH 29 - PROGRESS: at 54.16% examples, 57783 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:37:49: EPOCH 29 - PROGRESS: at 68.60% examples, 62453 words/s, in_qsize 12, out_qsize 0\n",
      "INFO - 21:37:50: EPOCH 29 - PROGRESS: at 76.10% examples, 60539 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:37:51: EPOCH 29 - PROGRESS: at 89.18% examples, 62250 words/s, in_qsize 21, out_qsize 1\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:37:52: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:37:52: EPOCH - 29 : training on 2627642 raw words (677199 effective words) took 9.8s, 68975 effective words/s\n",
      "INFO - 21:37:53: EPOCH 30 - PROGRESS: at 10.44% examples, 66985 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 21:37:54: EPOCH 30 - PROGRESS: at 20.86% examples, 65961 words/s, in_qsize 0, out_qsize 3\n",
      "INFO - 21:37:55: EPOCH 30 - PROGRESS: at 32.54% examples, 68840 words/s, in_qsize 0, out_qsize 1\n",
      "INFO - 21:37:56: EPOCH 30 - PROGRESS: at 37.21% examples, 58630 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 21:37:57: EPOCH 30 - PROGRESS: at 44.60% examples, 54893 words/s, in_qsize 22, out_qsize 0\n",
      "INFO - 21:37:58: EPOCH 30 - PROGRESS: at 56.83% examples, 57334 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:37:59: EPOCH 30 - PROGRESS: at 66.58% examples, 58009 words/s, in_qsize 21, out_qsize 2\n",
      "INFO - 21:38:00: EPOCH 30 - PROGRESS: at 78.33% examples, 59906 words/s, in_qsize 21, out_qsize 0\n",
      "INFO - 21:38:01: EPOCH 30 - PROGRESS: at 91.48% examples, 62956 words/s, in_qsize 20, out_qsize 2\n",
      "INFO - 21:38:01: worker thread finished; awaiting finish of 10 more threads\n",
      "INFO - 21:38:01: worker thread finished; awaiting finish of 9 more threads\n",
      "INFO - 21:38:01: worker thread finished; awaiting finish of 8 more threads\n",
      "INFO - 21:38:02: worker thread finished; awaiting finish of 7 more threads\n",
      "INFO - 21:38:02: worker thread finished; awaiting finish of 6 more threads\n",
      "INFO - 21:38:02: worker thread finished; awaiting finish of 5 more threads\n",
      "INFO - 21:38:02: worker thread finished; awaiting finish of 4 more threads\n",
      "INFO - 21:38:02: worker thread finished; awaiting finish of 3 more threads\n",
      "INFO - 21:38:02: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 21:38:02: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 21:38:02: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 21:38:02: EPOCH - 30 : training on 2627642 raw words (678265 effective words) took 9.9s, 68230 effective words/s\n",
      "INFO - 21:38:02: training on a 78829260 raw words (20315408 effective words) took 301.3s, 67426 effective words/s\n",
      "INFO - 21:38:02: precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 5.02 mins\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=30)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
    "\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:38:02: saving Word2Vec object under word2vec.model, separately None\n",
      "INFO - 21:38:02: not storing attribute vectors_norm\n",
      "INFO - 21:38:02: not storing attribute cum_table\n",
      "INFO - 21:38:02: saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7202016</td>\n",
       "      <td>38917982</td>\n",
       "      <td>Cute and cozy place. Perfect location to every...</td>\n",
       "      <td>[cute, cozy, place, perfect, location, everyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39087409</td>\n",
       "      <td>Kelly has a great room in a very central locat...</td>\n",
       "      <td>[kelly, great, room, central, location, beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39820030</td>\n",
       "      <td>Very spacious apartment, and in a great neighb...</td>\n",
       "      <td>[spacious, apartment, great, neighborhood, kin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7202016</td>\n",
       "      <td>40813543</td>\n",
       "      <td>Close to Seattle Center and all it has to offe...</td>\n",
       "      <td>[close, seattle, center, offer, ballet, theate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7202016</td>\n",
       "      <td>41986501</td>\n",
       "      <td>Kelly was a great host and very accommodating ...</td>\n",
       "      <td>[kelly, great, host, accommodating, great, nei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        id                                           comments  \\\n",
       "0     7202016  38917982  Cute and cozy place. Perfect location to every...   \n",
       "1     7202016  39087409  Kelly has a great room in a very central locat...   \n",
       "2     7202016  39820030  Very spacious apartment, and in a great neighb...   \n",
       "3     7202016  40813543  Close to Seattle Center and all it has to offe...   \n",
       "4     7202016  41986501  Kelly was a great host and very accommodating ...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [cute, cozy, place, perfect, location, everyth...  \n",
       "1  [kelly, great, room, central, location, beauti...  \n",
       "2  [spacious, apartment, great, neighborhood, kin...  \n",
       "3  [close, seattle, center, offer, ballet, theate...  \n",
       "4  [kelly, great, host, accommodating, great, nei...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['date','reviewer_id','reviewer_name','clean_comment','tokenized','no_stopwords']\n",
    "\n",
    "df.drop(columns=cols,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7202016</td>\n",
       "      <td>38917982</td>\n",
       "      <td>Cute and cozy place. Perfect location to every...</td>\n",
       "      <td>[cute, cozy, place, perfect, location, everyth...</td>\n",
       "      <td>cute cozy place perfect location everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39087409</td>\n",
       "      <td>Kelly has a great room in a very central locat...</td>\n",
       "      <td>[kelly, great, room, central, location, beauti...</td>\n",
       "      <td>kelly great room central location beautiful bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39820030</td>\n",
       "      <td>Very spacious apartment, and in a great neighb...</td>\n",
       "      <td>[spacious, apartment, great, neighborhood, kin...</td>\n",
       "      <td>spacious apartment great neighborhood kind apa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7202016</td>\n",
       "      <td>40813543</td>\n",
       "      <td>Close to Seattle Center and all it has to offe...</td>\n",
       "      <td>[close, seattle, center, offer, ballet, theate...</td>\n",
       "      <td>close seattle center offer ballet theater_muse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7202016</td>\n",
       "      <td>41986501</td>\n",
       "      <td>Kelly was a great host and very accommodating ...</td>\n",
       "      <td>[kelly, great, host, accommodating, great, nei...</td>\n",
       "      <td>kelly great host accommodating great neighborh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        id                                           comments  \\\n",
       "0     7202016  38917982  Cute and cozy place. Perfect location to every...   \n",
       "1     7202016  39087409  Kelly has a great room in a very central locat...   \n",
       "2     7202016  39820030  Very spacious apartment, and in a great neighb...   \n",
       "3     7202016  40813543  Close to Seattle Center and all it has to offe...   \n",
       "4     7202016  41986501  Kelly was a great host and very accommodating ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [cute, cozy, place, perfect, location, everyth...   \n",
       "1  [kelly, great, room, central, location, beauti...   \n",
       "2  [spacious, apartment, great, neighborhood, kin...   \n",
       "3  [close, seattle, center, offer, ballet, theate...   \n",
       "4  [kelly, great, host, accommodating, great, nei...   \n",
       "\n",
       "                                               clean  \n",
       "0        cute cozy place perfect location everything  \n",
       "1  kelly great room central location beautiful bu...  \n",
       "2  spacious apartment great neighborhood kind apa...  \n",
       "3  close seattle center offer ballet theater_muse...  \n",
       "4  kelly great host accommodating great neighborh...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean'] = df['lemmatized'].apply(lambda x: ' '.join(bigram[x]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) K-means Clustering\n",
    "- basic technique for unsupervised clustering\n",
    "- set n=2 (we want positive vs negative)\n",
    "- 50 starting points to prevent wrong choice of centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 21:47:54: loading Word2Vec object from word2vec.model\n",
      "INFO - 21:47:55: loading wv recursively from word2vec.model.wv.* with mmap=None\n",
      "INFO - 21:47:55: setting ignored attribute vectors_norm to None\n",
      "INFO - 21:47:55: loading vocabulary recursively from word2vec.model.vocabulary.* with mmap=None\n",
      "INFO - 21:47:55: loading trainables recursively from word2vec.model.trainables.* with mmap=None\n",
      "INFO - 21:47:55: setting ignored attribute cum_table to None\n",
      "INFO - 21:47:55: loaded word2vec.model\n"
     ]
    }
   ],
   "source": [
    "word_vectors = Word2Vec.load(\"word2vec.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors.astype('double'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## see closest words to each cluster centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wifi_unstable', 0.9956074953079224),\n",
       " ('biggest_complaint', 0.9948632717132568),\n",
       " ('old_nasty', 0.9943544864654541),\n",
       " ('might_appeal', 0.9931325912475586),\n",
       " ('insect_repellent', 0.9929763674736023),\n",
       " ('noggin', 0.9908583164215088),\n",
       " ('wiring', 0.9907512664794922),\n",
       " ('kept_banging', 0.9907026290893555),\n",
       " ('goo', 0.9895851016044617),\n",
       " ('flashing', 0.9888520836830139),\n",
       " ('lever', 0.9885170459747314),\n",
       " ('somebody_el', 0.9883679151535034),\n",
       " ('unsanitary', 0.9881523847579956),\n",
       " ('litter', 0.988092303276062),\n",
       " ('hinge', 0.9875819087028503),\n",
       " ('need_updating', 0.987217366695404),\n",
       " ('plastic_bag', 0.9865788221359253),\n",
       " ('wash_hand', 0.9864225387573242),\n",
       " ('investment', 0.9863881468772888),\n",
       " ('luke_warm', 0.9862217903137207)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cluster 1 is words with negative sentiments\n",
    "word_vectors.similar_by_vector(k_model.cluster_centers_[1], topn=20, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kay_denis', 0.9945439100265503),\n",
       " ('perfect_launchpad', 0.9936453700065613),\n",
       " ('give_pointer', 0.99309241771698),\n",
       " ('ruby_dog', 0.9930784702301025),\n",
       " ('become_favorite', 0.9929834008216858),\n",
       " ('saul_megans', 0.992965579032898),\n",
       " ('tinyhouse', 0.9926290512084961),\n",
       " ('sooooooo', 0.9922561645507812),\n",
       " ('qa_hill', 0.9921590089797974),\n",
       " ('erickas', 0.9920530915260315),\n",
       " ('aug_2013', 0.9920216798782349),\n",
       " ('quiet_neighborhoody', 0.9919600486755371),\n",
       " ('nc', 0.9918854236602783),\n",
       " ('without_invasive', 0.9918511509895325),\n",
       " ('uw_village', 0.9917807579040527),\n",
       " ('lifetime_experience', 0.9917296171188354),\n",
       " ('georgeous', 0.9916488528251648),\n",
       " ('unbelievable_hospitality', 0.9916225671768188),\n",
       " ('child_hospital', 0.9915664196014404),\n",
       " ('vaibhav_heidi', 0.9910863637924194)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cluster 0 is words with positive sentiments\n",
    "word_vectors.similar_by_vector(k_model.cluster_centers_[0], topn=20, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results from above\n",
    "- cluster for negative sentiment is pretty accurate, with most phrases linking to bad experiences\n",
    "- cluster for positive sentiment might not be that accurate, with some phrases having no relation to good experiences\n",
    "- but this might be because most of the sentences revolving around these phrases have been from good reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cluster = k_model.cluster_centers_[0]\n",
    "neg_cluster = k_model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign sentiment scores\n",
    "- based on how close the words are to centroid\n",
    "- (-1 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cute</td>\n",
       "      <td>[-0.024901101, -0.066744104, 0.010962429, -0.1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cozy</td>\n",
       "      <td>[0.01864702, -0.023149328, 0.041681483, -0.129...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>place</td>\n",
       "      <td>[-0.010262076, -0.023641733, 0.113858655, -0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perfect</td>\n",
       "      <td>[-0.020732332, -0.02959141, 0.11434515, -0.107...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>location</td>\n",
       "      <td>[-0.032389443, -0.059886683, 0.09468089, -0.03...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>everything</td>\n",
       "      <td>[0.041884515, 0.026371881, 0.045442346, -0.070...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kelly</td>\n",
       "      <td>[0.010439747, 0.121025056, 0.054996, -0.085330...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>great</td>\n",
       "      <td>[-0.04261457, -0.0873449, 0.048071723, -0.0137...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>room</td>\n",
       "      <td>[0.06447234, -0.060898483, 0.0531486, -0.00776...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>central</td>\n",
       "      <td>[-0.019869706, -0.090317436, 0.085564695, 0.01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>[-0.023554264, -0.07231123, 0.05005029, -0.087...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>building</td>\n",
       "      <td>[0.030707467, -0.12227987, 0.012342717, 0.0588...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>architecture</td>\n",
       "      <td>[-0.05733032, -0.09546977, 0.005626723, -0.076...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>style</td>\n",
       "      <td>[-0.04268621, -0.031591725, 0.0040896796, -0.1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>really</td>\n",
       "      <td>[0.025264055, -0.08752678, 0.013747934, -0.041...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words                                            vectors  cluster\n",
       "0           cute  [-0.024901101, -0.066744104, 0.010962429, -0.1...        0\n",
       "1           cozy  [0.01864702, -0.023149328, 0.041681483, -0.129...        0\n",
       "2          place  [-0.010262076, -0.023641733, 0.113858655, -0.0...        0\n",
       "3        perfect  [-0.020732332, -0.02959141, 0.11434515, -0.107...        0\n",
       "4       location  [-0.032389443, -0.059886683, 0.09468089, -0.03...        0\n",
       "5     everything  [0.041884515, 0.026371881, 0.045442346, -0.070...        0\n",
       "6          kelly  [0.010439747, 0.121025056, 0.054996, -0.085330...        0\n",
       "7          great  [-0.04261457, -0.0873449, 0.048071723, -0.0137...        0\n",
       "8           room  [0.06447234, -0.060898483, 0.0531486, -0.00776...        1\n",
       "9        central  [-0.019869706, -0.090317436, 0.085564695, 0.01...        0\n",
       "10     beautiful  [-0.023554264, -0.07231123, 0.05005029, -0.087...        0\n",
       "11      building  [0.030707467, -0.12227987, 0.012342717, 0.0588...        1\n",
       "12  architecture  [-0.05733032, -0.09546977, 0.005626723, -0.076...        0\n",
       "13         style  [-0.04268621, -0.031591725, 0.0040896796, -0.1...        1\n",
       "14        really  [0.025264055, -0.08752678, 0.013747934, -0.041...        0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df = pd.DataFrame(word_vectors.vocab.keys())\n",
    "word_df.columns = ['words']\n",
    "word_df['vectors'] = word_df['words'].apply(lambda x: word_vectors[f'{x}'])\n",
    "word_df['cluster'] = word_df['vectors'].apply(lambda x: k_model.predict([np.array(x)]))\n",
    "word_df['cluster'] = word_df['cluster'].apply(lambda x: x[0])\n",
    "word_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 22:16:11: Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO - 22:16:11: NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>label</th>\n",
       "      <th>distance</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cute</td>\n",
       "      <td>[-0.024901101, -0.066744104, 0.010962429, -0.1...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.080790</td>\n",
       "      <td>1.080790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cozy</td>\n",
       "      <td>[0.01864702, -0.023149328, 0.041681483, -0.129...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.092470</td>\n",
       "      <td>1.092470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>place</td>\n",
       "      <td>[-0.010262076, -0.023641733, 0.113858655, -0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.165105</td>\n",
       "      <td>1.165105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perfect</td>\n",
       "      <td>[-0.020732332, -0.02959141, 0.11434515, -0.107...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.168722</td>\n",
       "      <td>1.168722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>location</td>\n",
       "      <td>[-0.032389443, -0.059886683, 0.09468089, -0.03...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.090340</td>\n",
       "      <td>1.090340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>everything</td>\n",
       "      <td>[0.041884515, 0.026371881, 0.045442346, -0.070...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.051214</td>\n",
       "      <td>1.051214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kelly</td>\n",
       "      <td>[0.010439747, 0.121025056, 0.054996, -0.085330...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.175903</td>\n",
       "      <td>1.175903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>great</td>\n",
       "      <td>[-0.04261457, -0.0873449, 0.048071723, -0.0137...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.126342</td>\n",
       "      <td>1.126342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>room</td>\n",
       "      <td>[0.06447234, -0.060898483, 0.0531486, -0.00776...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.982031</td>\n",
       "      <td>-0.982031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>central</td>\n",
       "      <td>[-0.019869706, -0.090317436, 0.085564695, 0.01...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.074237</td>\n",
       "      <td>1.074237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>beautiful</td>\n",
       "      <td>[-0.023554264, -0.07231123, 0.05005029, -0.087...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.109777</td>\n",
       "      <td>1.109777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>building</td>\n",
       "      <td>[0.030707467, -0.12227987, 0.012342717, 0.0588...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.994154</td>\n",
       "      <td>-0.994154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>architecture</td>\n",
       "      <td>[-0.05733032, -0.09546977, 0.005626723, -0.076...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.982683</td>\n",
       "      <td>0.982683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>style</td>\n",
       "      <td>[-0.04268621, -0.031591725, 0.0040896796, -0.1...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.022741</td>\n",
       "      <td>-1.022741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>really</td>\n",
       "      <td>[0.025264055, -0.08752678, 0.013747934, -0.041...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.107902</td>\n",
       "      <td>1.107902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words                                            vectors  cluster  \\\n",
       "0           cute  [-0.024901101, -0.066744104, 0.010962429, -0.1...        0   \n",
       "1           cozy  [0.01864702, -0.023149328, 0.041681483, -0.129...        0   \n",
       "2          place  [-0.010262076, -0.023641733, 0.113858655, -0.0...        0   \n",
       "3        perfect  [-0.020732332, -0.02959141, 0.11434515, -0.107...        0   \n",
       "4       location  [-0.032389443, -0.059886683, 0.09468089, -0.03...        0   \n",
       "5     everything  [0.041884515, 0.026371881, 0.045442346, -0.070...        0   \n",
       "6          kelly  [0.010439747, 0.121025056, 0.054996, -0.085330...        0   \n",
       "7          great  [-0.04261457, -0.0873449, 0.048071723, -0.0137...        0   \n",
       "8           room  [0.06447234, -0.060898483, 0.0531486, -0.00776...        1   \n",
       "9        central  [-0.019869706, -0.090317436, 0.085564695, 0.01...        0   \n",
       "10     beautiful  [-0.023554264, -0.07231123, 0.05005029, -0.087...        0   \n",
       "11      building  [0.030707467, -0.12227987, 0.012342717, 0.0588...        1   \n",
       "12  architecture  [-0.05733032, -0.09546977, 0.005626723, -0.076...        0   \n",
       "13         style  [-0.04268621, -0.031591725, 0.0040896796, -0.1...        1   \n",
       "14        really  [0.025264055, -0.08752678, 0.013747934, -0.041...        0   \n",
       "\n",
       "    label  distance  sentiment  \n",
       "0       1  1.080790   1.080790  \n",
       "1       1  1.092470   1.092470  \n",
       "2       1  1.165105   1.165105  \n",
       "3       1  1.168722   1.168722  \n",
       "4       1  1.090340   1.090340  \n",
       "5       1  1.051214   1.051214  \n",
       "6       1  1.175903   1.175903  \n",
       "7       1  1.126342   1.126342  \n",
       "8      -1  0.982031  -0.982031  \n",
       "9       1  1.074237   1.074237  \n",
       "10      1  1.109777   1.109777  \n",
       "11     -1  0.994154  -0.994154  \n",
       "12      1  0.982683   0.982683  \n",
       "13     -1  1.022741  -1.022741  \n",
       "14      1  1.107902   1.107902  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df['label'] = [1 if i==0 else -1 for i in word_df.cluster]\n",
    "word_df['distance'] = word_df.apply(lambda x: 1/(k_model.transform([x.vectors]).min()), axis=1)\n",
    "\n",
    "# sentiment score = distance from centroid x centroid value\n",
    "word_df['sentiment'] = word_df['distance'] * word_df['label']\n",
    "word_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## next steps\n",
    "- now that we have sentiment scores for each word, we can use it to predict sentiment score of sentences\n",
    "- possible method of improvement - 3rd cluster for neutral words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Predict Sentiment Scores of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7202016</td>\n",
       "      <td>38917982</td>\n",
       "      <td>Cute and cozy place. Perfect location to every...</td>\n",
       "      <td>[cute, cozy, place, perfect, location, everyth...</td>\n",
       "      <td>cute cozy place perfect location everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39087409</td>\n",
       "      <td>Kelly has a great room in a very central locat...</td>\n",
       "      <td>[kelly, great, room, central, location, beauti...</td>\n",
       "      <td>kelly great room central location beautiful bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7202016</td>\n",
       "      <td>39820030</td>\n",
       "      <td>Very spacious apartment, and in a great neighb...</td>\n",
       "      <td>[spacious, apartment, great, neighborhood, kin...</td>\n",
       "      <td>spacious apartment great neighborhood kind apa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7202016</td>\n",
       "      <td>40813543</td>\n",
       "      <td>Close to Seattle Center and all it has to offe...</td>\n",
       "      <td>[close, seattle, center, offer, ballet, theate...</td>\n",
       "      <td>close seattle center offer ballet theater_muse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7202016</td>\n",
       "      <td>41986501</td>\n",
       "      <td>Kelly was a great host and very accommodating ...</td>\n",
       "      <td>[kelly, great, host, accommodating, great, nei...</td>\n",
       "      <td>kelly great host accommodating great neighborh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        id                                           comments  \\\n",
       "0     7202016  38917982  Cute and cozy place. Perfect location to every...   \n",
       "1     7202016  39087409  Kelly has a great room in a very central locat...   \n",
       "2     7202016  39820030  Very spacious apartment, and in a great neighb...   \n",
       "3     7202016  40813543  Close to Seattle Center and all it has to offe...   \n",
       "4     7202016  41986501  Kelly was a great host and very accommodating ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [cute, cozy, place, perfect, location, everyth...   \n",
       "1  [kelly, great, room, central, location, beauti...   \n",
       "2  [spacious, apartment, great, neighborhood, kin...   \n",
       "3  [close, seattle, center, offer, ballet, theate...   \n",
       "4  [kelly, great, host, accommodating, great, nei...   \n",
       "\n",
       "                                               clean  \n",
       "0        cute cozy place perfect location everything  \n",
       "1  kelly great room central location beautiful bu...  \n",
       "2  spacious apartment great neighborhood kind apa...  \n",
       "3  close seattle center offer ballet theater_muse...  \n",
       "4  kelly great host accommodating great neighborh...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df = df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>label</th>\n",
       "      <th>distance</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cute</td>\n",
       "      <td>[-0.024901101, -0.066744104, 0.010962429, -0.1...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.080790</td>\n",
       "      <td>1.080790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cozy</td>\n",
       "      <td>[0.01864702, -0.023149328, 0.041681483, -0.129...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.092470</td>\n",
       "      <td>1.092470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>place</td>\n",
       "      <td>[-0.010262076, -0.023641733, 0.113858655, -0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.165105</td>\n",
       "      <td>1.165105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>perfect</td>\n",
       "      <td>[-0.020732332, -0.02959141, 0.11434515, -0.107...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.168722</td>\n",
       "      <td>1.168722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>location</td>\n",
       "      <td>[-0.032389443, -0.059886683, 0.09468089, -0.03...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.090340</td>\n",
       "      <td>1.090340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      words                                            vectors  cluster  \\\n",
       "0      cute  [-0.024901101, -0.066744104, 0.010962429, -0.1...        0   \n",
       "1      cozy  [0.01864702, -0.023149328, 0.041681483, -0.129...        0   \n",
       "2     place  [-0.010262076, -0.023641733, 0.113858655, -0.0...        0   \n",
       "3   perfect  [-0.020732332, -0.02959141, 0.11434515, -0.107...        0   \n",
       "4  location  [-0.032389443, -0.059886683, 0.09468089, -0.03...        0   \n",
       "\n",
       "   label  distance  sentiment  \n",
       "0      1  1.080790   1.080790  \n",
       "1      1  1.092470   1.092470  \n",
       "2      1  1.165105   1.165105  \n",
       "3      1  1.168722   1.168722  \n",
       "4      1  1.090340   1.090340  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df = word_df\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_map = dict(zip(sentiment_df['words'].values, sentiment_df['sentiment'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tf-idf scores for words in each sentence\n",
    "- tf-idf weightage is based on uniqueness of word\n",
    "- more unique the word across corpus = higher score\n",
    "- hence, more unique words will contribute a higher weightage to the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Boon Kong\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "tfidf.fit(main_df['clean'])\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "transformed = tfidf.transform(main_df['clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of key (word) to value (tf-idf score)\n",
    "def create_tfidf(x, transformed_file, features):\n",
    "    vector = transformed_file[x.name].tocoo() # matrix to coordinates\n",
    "    vector.col = features.iloc[vector.col].values\n",
    "    coo_dict = dict(zip(vector.col, vector.data))\n",
    "    return coo_dict\n",
    "\n",
    "# replace word with tf-idf score\n",
    "def word_to_tfidf(x, transformed_file, features):\n",
    "    dictionary = create_tfidf(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x['clean'].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# may take up to 5mins\n",
    "tfidf_scores = main_df.apply(lambda x: word_to_tfidf(x, transformed, features), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing sentiment scores into words in each sentence\n",
    "- these scores were created during K-means clustering, now you are just putting them into sentence format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_sentiment(word, sentiment_dict):\n",
    "    try:\n",
    "        output = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        output = 0\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_scores = main_df['clean'].apply(lambda x: list(map(lambda y: word_to_sentiment(y, sent_map), x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0807900676706315, 1.0924704996364492, 1.165...</td>\n",
       "      <td>[4.334304533362949, 3.4299899925355093, 1.9144...</td>\n",
       "      <td>cute cozy place perfect location everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.1759034646294253, 1.1263419456038901, -0.98...</td>\n",
       "      <td>[6.897601817096672, 3.384974820083783, 2.66101...</td>\n",
       "      <td>kelly great room central location beautiful bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0205340292759995, 1.0306948059770953, 1.126...</td>\n",
       "      <td>[3.89758688175068, 5.134640704200904, 1.692487...</td>\n",
       "      <td>spacious apartment great neighborhood kind apa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.04017413501294, 1.1063623947429364, 0.97384...</td>\n",
       "      <td>[3.0789937927887823, 2.04520279070619, 5.17544...</td>\n",
       "      <td>close seattle center offer ballet theater_muse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.1759034646294253, 1.1263419456038901, 1.164...</td>\n",
       "      <td>[6.897601817096672, 6.769949640167566, 2.21166...</td>\n",
       "      <td>kelly great host accommodating great neighborh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     sentiment_score  \\\n",
       "0  [1.0807900676706315, 1.0924704996364492, 1.165...   \n",
       "1  [1.1759034646294253, 1.1263419456038901, -0.98...   \n",
       "2  [1.0205340292759995, 1.0306948059770953, 1.126...   \n",
       "3  [1.04017413501294, 1.1063623947429364, 0.97384...   \n",
       "4  [1.1759034646294253, 1.1263419456038901, 1.164...   \n",
       "\n",
       "                                         tfidf_score  \\\n",
       "0  [4.334304533362949, 3.4299899925355093, 1.9144...   \n",
       "1  [6.897601817096672, 3.384974820083783, 2.66101...   \n",
       "2  [3.89758688175068, 5.134640704200904, 1.692487...   \n",
       "3  [3.0789937927887823, 2.04520279070619, 5.17544...   \n",
       "4  [6.897601817096672, 6.769949640167566, 2.21166...   \n",
       "\n",
       "                                            sentence  \n",
       "0        cute cozy place perfect location everything  \n",
       "1  kelly great room central location beautiful bu...  \n",
       "2  spacious apartment great neighborhood kind apa...  \n",
       "3  close seattle center offer ballet theater_muse...  \n",
       "4  kelly great host accommodating great neighborh...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(data=[sentiment_scores, tfidf_scores, main_df['clean']]).T\n",
    "final_df.columns = ['sentiment_score', 'tfidf_score', 'sentence']\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge scores \n",
    "- dot product of sentiment and tf-idf scores = overall sentiment\n",
    "- positive (>0), negative (<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment_rate</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1.0807900676706315, 1.0924704996364492, 1.165...</td>\n",
       "      <td>[4.334304533362949, 3.4299899925355093, 1.9144...</td>\n",
       "      <td>cute cozy place perfect location everything</td>\n",
       "      <td>19.245867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1.1759034646294253, 1.1263419456038901, -0.98...</td>\n",
       "      <td>[6.897601817096672, 3.384974820083783, 2.66101...</td>\n",
       "      <td>kelly great room central location beautiful bu...</td>\n",
       "      <td>108.777278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1.0205340292759995, 1.0306948059770953, 1.126...</td>\n",
       "      <td>[3.89758688175068, 5.134640704200904, 1.692487...</td>\n",
       "      <td>spacious apartment great neighborhood kind apa...</td>\n",
       "      <td>56.740638</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1.04017413501294, 1.1063623947429364, 0.97384...</td>\n",
       "      <td>[3.0789937927887823, 2.04520279070619, 5.17544...</td>\n",
       "      <td>close seattle center offer ballet theater_muse...</td>\n",
       "      <td>131.140522</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1.1759034646294253, 1.1263419456038901, 1.164...</td>\n",
       "      <td>[6.897601817096672, 6.769949640167566, 2.21166...</td>\n",
       "      <td>kelly great host accommodating great neighborh...</td>\n",
       "      <td>121.286061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1.1759034646294253, 1.1263419456038901, 1.165...</td>\n",
       "      <td>[6.897601817096672, 3.384974820083783, 3.82893...</td>\n",
       "      <td>kelly great place great looking clean simple w...</td>\n",
       "      <td>39.370889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1.1759034646294253, 1.1263419456038901, 1.020...</td>\n",
       "      <td>[6.897601817096672, 3.384974820083783, 2.49734...</td>\n",
       "      <td>kelly great nice neighborhood place stay expec...</td>\n",
       "      <td>25.369374</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[0, 1.144072044336585, 0, -0.9620808091281017,...</td>\n",
       "      <td>[11.250027981994263, 7.122893596949171, 11.655...</td>\n",
       "      <td>hola bnb erz left seattle simply fantastic tim...</td>\n",
       "      <td>13.834402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1.1759034646294253, 1.165105090604758, 1.1086...</td>\n",
       "      <td>[13.795203634193344, 1.914465645264698, 5.2338...</td>\n",
       "      <td>kelly place conveniently_located quiet street ...</td>\n",
       "      <td>73.895394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1.165105090604758, 1.1079019359329356, 1.0201...</td>\n",
       "      <td>[1.914465645264698, 3.1014859255634732, 2.4973...</td>\n",
       "      <td>place really nice clean important_aspect close...</td>\n",
       "      <td>-117.702295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[1.165105090604758, 1.1079019359329356, 1.0201...</td>\n",
       "      <td>[1.914465645264698, 3.1014859255634732, 2.4973...</td>\n",
       "      <td>place really nice clean quiet night clean line...</td>\n",
       "      <td>79.280110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[1.0839125405290801, 1.18741065601558, 1.17590...</td>\n",
       "      <td>[4.8240790443770125, 4.992999167688119, 6.8976...</td>\n",
       "      <td>listing exactly_described kelly place wonderfu...</td>\n",
       "      <td>102.322509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[1.1595909887630853, 1.1433905127858528, 1.165...</td>\n",
       "      <td>[3.5338641821170915, 7.392813213061111, 1.9144...</td>\n",
       "      <td>welcoming nicer place live seattle area</td>\n",
       "      <td>26.456935</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[1.1263419456038901, 1.0903400309923168, 1.126...</td>\n",
       "      <td>[5.077462230125674, 2.1541262444240736, 5.0774...</td>\n",
       "      <td>great location great price great host thanks w...</td>\n",
       "      <td>52.493549</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[1.2788995881932848, 1.1759034646294253, 1.036...</td>\n",
       "      <td>[3.2370158716253474, 6.897601817096672, 2.9825...</td>\n",
       "      <td>staying kelly easy location block_away public_...</td>\n",
       "      <td>-8.137696</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sentiment_score  \\\n",
       "0   [1.0807900676706315, 1.0924704996364492, 1.165...   \n",
       "1   [1.1759034646294253, 1.1263419456038901, -0.98...   \n",
       "2   [1.0205340292759995, 1.0306948059770953, 1.126...   \n",
       "3   [1.04017413501294, 1.1063623947429364, 0.97384...   \n",
       "4   [1.1759034646294253, 1.1263419456038901, 1.164...   \n",
       "5   [1.1759034646294253, 1.1263419456038901, 1.165...   \n",
       "6   [1.1759034646294253, 1.1263419456038901, 1.020...   \n",
       "7   [0, 1.144072044336585, 0, -0.9620808091281017,...   \n",
       "8   [1.1759034646294253, 1.165105090604758, 1.1086...   \n",
       "9   [1.165105090604758, 1.1079019359329356, 1.0201...   \n",
       "10  [1.165105090604758, 1.1079019359329356, 1.0201...   \n",
       "11  [1.0839125405290801, 1.18741065601558, 1.17590...   \n",
       "12  [1.1595909887630853, 1.1433905127858528, 1.165...   \n",
       "13  [1.1263419456038901, 1.0903400309923168, 1.126...   \n",
       "14  [1.2788995881932848, 1.1759034646294253, 1.036...   \n",
       "\n",
       "                                          tfidf_score  \\\n",
       "0   [4.334304533362949, 3.4299899925355093, 1.9144...   \n",
       "1   [6.897601817096672, 3.384974820083783, 2.66101...   \n",
       "2   [3.89758688175068, 5.134640704200904, 1.692487...   \n",
       "3   [3.0789937927887823, 2.04520279070619, 5.17544...   \n",
       "4   [6.897601817096672, 6.769949640167566, 2.21166...   \n",
       "5   [6.897601817096672, 3.384974820083783, 3.82893...   \n",
       "6   [6.897601817096672, 3.384974820083783, 2.49734...   \n",
       "7   [11.250027981994263, 7.122893596949171, 11.655...   \n",
       "8   [13.795203634193344, 1.914465645264698, 5.2338...   \n",
       "9   [1.914465645264698, 3.1014859255634732, 2.4973...   \n",
       "10  [1.914465645264698, 3.1014859255634732, 2.4973...   \n",
       "11  [4.8240790443770125, 4.992999167688119, 6.8976...   \n",
       "12  [3.5338641821170915, 7.392813213061111, 1.9144...   \n",
       "13  [5.077462230125674, 2.1541262444240736, 5.0774...   \n",
       "14  [3.2370158716253474, 6.897601817096672, 2.9825...   \n",
       "\n",
       "                                             sentence  sentiment_rate  \\\n",
       "0         cute cozy place perfect location everything       19.245867   \n",
       "1   kelly great room central location beautiful bu...      108.777278   \n",
       "2   spacious apartment great neighborhood kind apa...       56.740638   \n",
       "3   close seattle center offer ballet theater_muse...      131.140522   \n",
       "4   kelly great host accommodating great neighborh...      121.286061   \n",
       "5   kelly great place great looking clean simple w...       39.370889   \n",
       "6   kelly great nice neighborhood place stay expec...       25.369374   \n",
       "7   hola bnb erz left seattle simply fantastic tim...       13.834402   \n",
       "8   kelly place conveniently_located quiet street ...       73.895394   \n",
       "9   place really nice clean important_aspect close...     -117.702295   \n",
       "10  place really nice clean quiet night clean line...       79.280110   \n",
       "11  listing exactly_described kelly place wonderfu...      102.322509   \n",
       "12            welcoming nicer place live seattle area       26.456935   \n",
       "13  great location great price great host thanks w...       52.493549   \n",
       "14  staying kelly easy location block_away public_...       -8.137696   \n",
       "\n",
       "    prediction  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "5            1  \n",
       "6            1  \n",
       "7            1  \n",
       "8            1  \n",
       "9            0  \n",
       "10           1  \n",
       "11           1  \n",
       "12           1  \n",
       "13           1  \n",
       "14           0  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(data=[sentiment_scores, tfidf_scores, main_df['clean']]).T\n",
    "final_df.columns = ['sentiment_score', 'tfidf_score', 'sentence']\n",
    "\n",
    "# dot product\n",
    "final_df['sentiment_rate'] = final_df.apply(lambda x: np.array(x.loc['sentiment_score']) @ np.array(x.loc['tfidf_score']), axis=1)\n",
    "final_df['prediction'] = (final_df['sentiment_rate']>0).astype('int8')\n",
    "final_df.head(15)\n",
    "# final_df['sentiment'] = [1 if i==1 else 0 for i in final_df['sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = final_df['prediction']\n",
    "y_test = final_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9384</td>\n",
       "      <td>75465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1\n",
       "0  9384  75465\n",
       "1     0      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix = pd.DataFrame(confusion_matrix(y_test, y_pred))\n",
    "print('This is the Confusion Matrix:')\n",
    "display(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is an unsupervised method, there is no labels for checking the accuracy, precision or recall of the model. Upon study of the model output, there is a strong dominance of positive reviews. Many reviews with a slight hint of negativity is still labelled as positive, hence this model is not good enough for sentiment classification.\n",
    "\n",
    "\n",
    "Next, we will build a supervised model with the state-of-the-art BERT transformer, which will give us a better prediction of the sentiment. Please refer to the BERT_sentiment notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
